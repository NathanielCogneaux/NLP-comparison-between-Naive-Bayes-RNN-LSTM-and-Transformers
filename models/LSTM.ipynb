{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4b1f4ce6-cb19-467b-9763-77a9f5544fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10896185-8198-4dab-8f51-669ce583e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4846, 2)\n",
      "  sentiment                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sentiment_analysis_for_financial_news_[from_kaggle].csv', encoding='ISO-8859-1',names=['sentiment', 'text'])\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "#drop rows with NaN in sentiment and text columns\n",
    "df = df.dropna(subset=['sentiment', 'text'])\n",
    "\n",
    "#convert sentiment to numerical labels\n",
    "df['sentiment'] = df['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "\n",
    "#check if dataset size is sufficient\n",
    "if len(df) == 0:\n",
    "    raise ValueError(\"After preprocessing, the dataset is empty. Please check the preprocessing steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9680a46-219a-4cff-9477-587d082b80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text), re.I|re.A)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if not word in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "#check if any text is empty after preprocessing\n",
    "empty_texts = df[df['text'] == '']\n",
    "if not empty_texts.empty:\n",
    "    print(f\"There are {len(empty_texts)} rows with empty text after preprocessing.\")\n",
    "    print(empty_texts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1388b2c1-a375-4b49-9239-5f1a45d8b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and padding\n",
    "class Tokenizer:\n",
    "    def __init__(self, max_features, maxlen):\n",
    "        self.max_features = max_features\n",
    "        self.maxlen = maxlen\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_counts.update(words)\n",
    "        common_words = word_counts.most_common(self.max_features)\n",
    "        self.word_index = {word: idx + 1 for idx, (word, count) in enumerate(common_words)}\n",
    "        self.index_word = {idx + 1: word for word, idx in self.word_index.items()}\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            sequence = [self.word_index.get(word, 0) for word in words][:self.maxlen]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def pad_sequences(self, sequences):\n",
    "        padded_sequences = np.zeros((len(sequences), self.maxlen))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            if len(sequence) == 0:\n",
    "                continue  # skip empty sequences\n",
    "            padded_sequence = sequence[:self.maxlen]\n",
    "            padded_sequences[i, -len(padded_sequence):] = padded_sequence\n",
    "        return padded_sequences.astype(int)\n",
    "\n",
    "max_features = 10000  \n",
    "maxlen = 200 \n",
    "\n",
    "tokenizer = Tokenizer(max_features, maxlen)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "X = tokenizer.pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea442479-7318-4245-82eb-e859622c7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Dataset Shape: (3840, 200), (3840, 3)\n",
      "Final Testing Dataset Shape: (960, 200), (960, 3)\n"
     ]
    }
   ],
   "source": [
    "#convert labels to one-hot encoding\n",
    "y = np.zeros((len(df), 3))\n",
    "y[np.arange(len(df)), df['sentiment'].astype(int)] = 1\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 64  \n",
    "\n",
    "train_size = len(X_train) - (len(X_train) % batch_size)\n",
    "test_size = len(X_test) - (len(X_test) % batch_size)\n",
    "\n",
    "X_train = X_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "X_test = X_test[:test_size]\n",
    "y_test = y_test[:test_size]\n",
    "\n",
    "#check the final sizes after truncation\n",
    "print(f\"Final Training Dataset Shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Final Testing Dataset Shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59c57dcb-6680-48d4-b6a7-9d7e2184f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.float)\n",
    "\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b67d966b-1aac-48f0-8e38-e70b4c05c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(10001, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (attention): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define the LSTM model \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, no_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, no_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # hidden_dim * 2 because of bidirectional LSTM\n",
    "        \n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "  \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "   \n",
    "        attention_w = torch.softmax(self.attention(lstm_out).squeeze(), dim=-1).unsqueeze(-1)\n",
    "        attention_out = torch.sum(attention_w * lstm_out, dim=1)\n",
    "        \n",
    "        out = self.dropout(attention_out)\n",
    "        out = self.fc(out) \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.no_layers * 2, batch_size, self.hidden_dim).zero_(),  \n",
    "                  weight.new(self.no_layers * 2, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "#hyperparameters\n",
    "vocab_size = max_features + 1\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256  \n",
    "output_dim = 3  \n",
    "no_layers = 2  \n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, no_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d91342c4-431d-4b4a-a84c-56a567e6cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:09<00:00,  2.15s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 \tTraining Loss: 0.537652 \tTraining Acc: 0.742448 \tValidation Loss: 0.489264 \tValidation Acc: 0.763542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:06<00:00,  2.12s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20 \tTraining Loss: 0.473857 \tTraining Acc: 0.783160 \tValidation Loss: 0.465974 \tValidation Acc: 0.791667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:17<00:00,  2.29s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:12<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20 \tTraining Loss: 0.400843 \tTraining Acc: 0.830990 \tValidation Loss: 0.445963 \tValidation Acc: 0.797569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:12<00:00,  2.21s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20 \tTraining Loss: 0.307773 \tTraining Acc: 0.883333 \tValidation Loss: 0.477009 \tValidation Acc: 0.802431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:06<00:00,  2.11s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20 \tTraining Loss: 0.227857 \tTraining Acc: 0.919705 \tValidation Loss: 0.468795 \tValidation Acc: 0.816667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:05<00:00,  2.09s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20 \tTraining Loss: 0.163078 \tTraining Acc: 0.945139 \tValidation Loss: 0.611758 \tValidation Acc: 0.802431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:07<00:00,  2.12s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20 \tTraining Loss: 0.107654 \tTraining Acc: 0.967188 \tValidation Loss: 0.616793 \tValidation Acc: 0.804167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [02:07<00:00,  2.12s/it]\n",
      "100%|███████████████████████████████████████████| 15/15 [00:11<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20 \tTraining Loss: 0.089326 \tTraining Acc: 0.974132 \tValidation Loss: 0.649382 \tValidation Acc: 0.805208\n",
      "Early stopping after 8 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#loss and optimization functions\n",
    "lr = 0.001\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "#training params\n",
    "epochs = 20  \n",
    "patience = 5 \n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "best_model = None\n",
    "\n",
    "#train the model\n",
    "model.train()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    train_loss\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = torch.round(torch.sigmoid(output))\n",
    "        correct_tensor = pred.eq(labels.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        train_acc += np.mean(correct)\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_acc / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            h = tuple([e.data for e in h])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            output, h = model(inputs, h)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            pred = torch.round(torch.sigmoid(output))\n",
    "            correct_tensor = pred.eq(labels.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "            val_acc += np.mean(correct)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(test_loader.dataset)\n",
    "    val_acc = val_acc / len(test_loader)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{epochs} \\tTraining Loss: {train_loss:.6f} \\tTraining Acc: {train_acc:.6f} \\tValidation Loss: {val_loss:.6f} \\tValidation Acc: {val_acc:.6f}')\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3683bf2-3942-40a0-ba99-4d3a2a3b3c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
